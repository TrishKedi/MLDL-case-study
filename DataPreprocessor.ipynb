{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMlo+jDFgWs32NyAJltz0QP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TrishKedi/MLDL-case-study/blob/main/DataPreprocessor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0fCwEJNaZpF8",
        "outputId": "04088457-66a1-4a02-ce08-f3d8038b4e30"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "z0BqB9K0Z_BD"
      },
      "outputs": [],
      "source": [
        "# Step 2: Set the path to your ZIP file in Drive\n",
        "zip_path = '/content/drive/My Drive/ML_CASE_STUDY/dataset.zip'  # â† change this to match your zip file path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t8aeirqwaNs4",
        "outputId": "08972456-5cd2-43c7-a7fb-b1b0719e0e68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Dataset extracted to: /content\n"
          ]
        }
      ],
      "source": [
        "# Step 3: Unzip the file into a working directory\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "extract_dir = '/content'\n",
        "os.makedirs(extract_dir, exist_ok=True)\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_dir)\n",
        "\n",
        "print(\" Dataset extracted to:\", extract_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Ddo6XCAbl_FO",
        "outputId": "f514f4c9-05bd-4734-d9ec-3d1a830f8a83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/albumentations/__init__.py:28: UserWarning: A new version of Albumentations is available: '2.0.7' (you have '2.0.6'). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
            "  check_for_updates()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=3.52s)\n",
            "creating index...\n",
            "index created!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch of images: 10\n",
            "First target sample: {'boxes': tensor([[310.6250, 410.0000, 318.7500, 426.2500],\n",
            "        [370.6250, 434.3750, 375.0000, 443.1250],\n",
            "        [282.5000, 419.3750, 302.5000, 425.6250],\n",
            "        [568.7500, 396.2500, 579.3750, 405.6250],\n",
            "        [564.3750, 410.6250, 578.7500, 422.5000],\n",
            "        [308.7500, 452.5000, 335.0000, 473.7500],\n",
            "        [265.0000, 441.8750, 311.2500, 483.7500],\n",
            "        [383.7500, 384.3750, 566.8750, 496.2500],\n",
            "        [110.0000, 455.6250, 198.7500, 504.3750],\n",
            "        [179.3750, 448.1250, 293.7500, 521.2500],\n",
            "        [321.2500, 445.6250, 408.1250, 515.0000],\n",
            "        [335.0000, 443.7500, 363.7500, 450.0000],\n",
            "        [633.7500, 428.7500, 799.3750, 525.6250],\n",
            "        [605.6250, 436.8750, 619.3750, 480.6250],\n",
            "        [568.7500, 440.0000, 577.5000, 470.0000]]), 'labels': tensor([1, 1, 2, 2, 2, 4, 4, 8, 7, 4, 4, 4, 4, 3, 3]), 'image_id': tensor([18502])}\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "from PIL import Image\n",
        "from pycocotools.coco import COCO\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Handle Data preprocessing on the fly\n",
        "\"\"\"\n",
        "class DataPreProcessor(Dataset):\n",
        "    def __init__(self, image_dir, annotation_file, transforms=None, category_ids=None):\n",
        "        self.image_dir = image_dir\n",
        "        self.coco = COCO(annotation_file)\n",
        "        self.ids = list(sorted(self.coco.imgs.keys()))\n",
        "        self.transforms = transforms\n",
        "        self.category_ids = category_ids\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img_id = self.ids[index]\n",
        "        ann_ids = self.coco.getAnnIds(imgIds=img_id)\n",
        "        anns = self.coco.loadAnns(ann_ids)\n",
        "\n",
        "        # Filter out invalid boxes and categories not in remap (if specified)\n",
        "        if self.category_ids is not None:\n",
        "            anns = [ann for ann in anns if ann['category_id'] in self.category_ids and ann['iscrowd'] == 0]\n",
        "\n",
        "        img_info = self.coco.loadImgs(img_id)[0]\n",
        "        img_path = os.path.join(self.image_dir, img_info['file_name'])\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        image = np.array(image)\n",
        "\n",
        "        boxes = []\n",
        "        labels = []\n",
        "        for ann in anns:\n",
        "            x, y, w, h = ann['bbox']\n",
        "            if w > 1 and h > 1:\n",
        "                boxes.append([x, y, x + w, y + h])  # Convert to Pascal VOC format\n",
        "                labels.append(ann['category_id'])\n",
        "\n",
        "        if self.transforms:\n",
        "            transformed = self.transforms(image=image, bboxes=boxes, class_labels=labels)\n",
        "            image = transformed['image']\n",
        "            boxes = transformed['bboxes']\n",
        "            labels = transformed['class_labels']\n",
        "\n",
        "        # Prepare target dictionary\n",
        "        target = {}\n",
        "        target['boxes'] = torch.tensor(boxes, dtype=torch.float32)\n",
        "        target['labels'] = torch.tensor(labels, dtype=torch.int64)\n",
        "        target['image_id'] = torch.tensor([img_id])\n",
        "\n",
        "        return image, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ids)\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Apply the following transformations during training\n",
        "\n",
        "- Resize to max size of 800 while maintaining aspect ratio\n",
        "- Pad to 800x800 if needed\n",
        "- Horizontal flip with 50% probability\n",
        "- Random brightness and contrast\n",
        "- Optional blur\n",
        "- Normalize with ImageNet mean and std\n",
        "- Convert to PyTorch tensors\n",
        "\"\"\"\n",
        "def get_train_transforms():\n",
        "    return A.Compose([\n",
        "        A.LongestMaxSize(max_size=800),\n",
        "        A.PadIfNeeded(min_height=800, min_width=800, border_mode=0),\n",
        "        A.HorizontalFlip(p=0.5),\n",
        "        A.RandomBrightnessContrast(p=0.2),\n",
        "        A.Blur(p=0.1),\n",
        "        A.Normalize(mean=(0.485, 0.456, 0.406),\n",
        "                    std=(0.229, 0.224, 0.225)),\n",
        "        ToTensorV2()\n",
        "    ], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['class_labels']))\n",
        "\n",
        "\n",
        "# Custom collate_fn for DataLoader\n",
        "def collate_fn(batch):\n",
        "    images, targets = list(zip(*batch))\n",
        "    return list(images), list(targets)\n",
        "\n",
        "\n",
        "image_dir = \"/content/dataset/train/images\"\n",
        "annotation_file = \"/content/dataset/train/coco_annotations.json\"\n",
        "\n",
        "dataset = DataPreProcessor(\n",
        "    image_dir=image_dir,\n",
        "    annotation_file=annotation_file,\n",
        "    transforms=get_train_transforms(),\n",
        "    category_ids=[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "dataloader = DataLoader(\n",
        "    dataset,\n",
        "    batch_size=10,\n",
        "    shuffle=True,\n",
        "    num_workers=4,\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "# Test a batch\n",
        "for imgs, targets in dataloader:\n",
        "    print(\"Batch of images:\", len(imgs))\n",
        "    print(\"First target sample:\", targets[0])\n",
        "    break\n"
      ]
    }
  ]
}